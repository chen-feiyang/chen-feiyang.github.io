<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  
  <meta name="generator" content="Hugo 0.58.1" />

  
  <meta name="description" content="Some description">
  

  
  <link rel="apple-touch-icon" sizes="180x180" href="https://chen-feiyang.github.io/apple-touch-icon.png">

  
  <link rel="icon" type="image/png" sizes="32x32" href="https://chen-feiyang.github.io/favicon-32x32.png">

  
  <link rel="icon" type="image/png" sizes="16x16" href="https://chen-feiyang.github.io/favicon-16x16.png">

  
  <link rel="manifest" href="https://chen-feiyang.github.io/site.webmanifest">

  
  <link rel="mask-icon" href="https://chen-feiyang.github.io/safari-pinned-tab.svg" color="#5bbad5">

  <meta name="msapplication-TileColor" content="#da532c">

  <meta name="theme-color" content="#ffffff">

  
  <link rel="stylesheet" href="https://chen-feiyang.github.io/css/bootstrap.min.css" />

  
  <title>outline of Machine Learning机器学习的概要 | Feiyang Chen&#39;s Blog</title>
  

  <style>
body {
  min-width: 300px;
}

.custom-navbar {
  margin-bottom: 1em;
  height: 60px;
}

.custom-navbar a {
  display: inline-block; 
  padding: 18px 0;
  margin-right: 1em; 
  font-weight: bold; 
}

.custom-navbar a:hover,
.custom-navbar a:focus {
  text-decoration: none; 
}

@media print {
  .custom-navbar {
    display: none;
  }
}

article {
  padding-bottom: 1em;
}

img {
  max-width: 100%;
}


body {
  background-color: #ffffff;
}



body {
  color: #212529;
}



a {
  color: #212529;
}



a:hover,
a:focus {
  color: #212529;
}



.custom-navbar {
  background-color: #212529;
}



.custom-navbar a {
  color: rgba(255, 255, 255, 0.75);
}



.custom-navbar a:hover,
.custom-navbar a:focus {
  color: rgba(255, 255, 255, 1);
}



.container {
  max-width: 800px;
}



pre {
  display: block;
  padding: 9.5px;
  word-break: break-all;
  word-wrap: break-word;
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
}

pre code {
  padding: 0;
  font-size: inherit;
  color: inherit; 
  white-space: pre-wrap;
  background-color: transparent;
  border: none;
  border-radius: 0;
}

code {
  padding: 2px 4px;
  color: inherit; 
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
  font-size: .9em;
}



blockquote,
.blockquote {
  padding: 10px 20px;
  margin: 0 0 20px;
  font-size: 1em;
  border-left: 5px solid #6c757d;
}

</style>
</head>

<script type="text/javascript"
        async
        src="https://cdn.bootcss.com/mathjax/2.7.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[\[','\]\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});

MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<style>
code.has-jax {
    font: inherit;
    font-size: 100%;
    background: inherit;
    border: inherit;
    color: #515151;
}
</style>

<body>
  <nav class="custom-navbar">
  <div class="container">
    
    <a href="/">Posts</a>
    
    <a href="/tags/">Tags</a>
    
    <a href="/about/">About</a>
    
    <a href="/index.xml">RSS</a>
    
  </div>
</nav>
  
  <div class="container">
    <article>
      <h1>outline of Machine Learning机器学习的概要</h1>


<p>The following outline is provided as an overview of and topical guide to machine learning.</p>

<p>下面的概要是机器学习的概述和主题指南。</p>

<p><strong>Machine learning</strong> – subfield of computer science<a href="more particularly soft computing">1</a> that evolved from the study of pattern recognition and computational learning theory in artificial intelligence.[1] In 1959, Arthur Samuel defined machine learning as a &quot;Field of study that gives computers the ability to learn without being explicitly programmed&quot;.[2] Machine learning explores the study and construction of algorithms that can learn from and make predictions on data.[3] Such algorithms operate by building a model from an example training set of input observations in order to make data-driven predictions or decisions expressed as outputs, rather than following strictly static program instructions.</p>

<p>机器学习-计算机科学（更确切地说软计算），它是从人工智能的模式识别和计算学习理论的研究发展而来的。1959年，阿瑟•塞缪尔将机器学习定义为“一门研究领域，它能让计算机在没有明确编程的情况下学习”。机器学习探索的是研究和构建可以学习和预测数据的算法。这种算法用样本训练集建立模型，以便做出数据驱动的预测或决定，并将其表示为输出，而不是严格遵循静态程序指令。</p>

<h2 id="1-what-type-of-thing-is-machine-learning">1 What type of thing is machine learning?</h2>

<ul>
<li>An academic discipline</li>
<li>A branch of science

<ul>
<li>An applied science

<ul>
<li>A subfield of computer science</li>
<li>A branch of artificial intelligence</li>
<li>A subfield of soft computing</li>
</ul></li>
</ul></li>
</ul>

<h2 id="2-branches-of-machine-learning">2 Branches of machine learning</h2>

<h3 id="2-1-subfields-of-machine-learning">2.1 Subfields of machine learning</h3>

<ul>
<li>Computational learning theory – studying the design and analysis of machine learning algorithms.[4]</li>
<li>Grammar induction</li>
<li>Meta learning</li>
</ul>

<h3 id="2-2-cross-disciplinary-fields-involving-machine-learning">2.2 Cross-disciplinary fields involving machine learning</h3>

<ul>
<li>Adversarial machine learning</li>
<li>Predictive analytics</li>
<li>Quantum machine learning</li>
<li>Robot learning

<ul>
<li>Developmental robotics</li>
</ul></li>
</ul>

<h2 id="3-applications-of-machine-learning">3 Applications of machine learning</h2>

<ul>
<li>Biomedical informatics</li>
<li>Computer vision</li>
<li>Customer relationship management –</li>
<li>Data mining</li>
<li>Email filtering</li>
<li>Inverted pendulum – balance and equilibrium system.</li>
<li>Natural language processing (NLP)

<ul>
<li>Automatic summarization</li>
<li>Automatic taxonomy construction</li>
<li>Dialog system</li>
<li>Grammar checker</li>
<li>Language recognition

<ul>
<li>Handwriting recognition</li>
<li>Optical character recognition</li>
<li>Speech recognition</li>
</ul></li>
<li>Machine translation</li>
<li>Question answering</li>
<li>Speech synthesis</li>
<li>Text mining

<ul>
<li>Term frequency–inverse document frequency (tf–idf)</li>
</ul></li>
<li>Text simplification</li>
</ul></li>
<li>Pattern recognition

<ul>
<li>Facial recognition system</li>
<li>Handwriting recognition</li>
<li>Image recognition</li>
<li>Optical character recognition</li>
<li>Speech recognition</li>
</ul></li>
<li>Recommendation system

<ul>
<li>Collaborative filtering</li>
<li>Content-based filtering</li>
<li>Hybrid recommender systems (Collaborative and content-based filtering)</li>
</ul></li>
<li>Search engine

<ul>
<li>Search engine optimization</li>
</ul></li>
</ul>

<h2 id="4-machine-learning-hardware">4 Machine learning hardware</h2>

<ul>
<li>Graphics processing unit</li>
<li>Tensor processing unit</li>
<li>Vision processing unit</li>
</ul>

<h2 id="5-machine-learning-tools">5 Machine learning tools</h2>

<ul>
<li>Comparison of deep learning software

<ul>
<li>Comparison of deep learning software/Resources</li>
</ul></li>
</ul>

<h3 id="5-1-machine-learning-frameworks">5.1 Machine learning frameworks</h3>

<p><strong>Proprietary machine learning frameworks</strong>
 - Amazon Machine Learning
 - Microsoft Azure Machine Learning Studio
 - DistBelief – replaced by TensorFlow
<strong>Open source machine learning frameworks</strong>
 - Apache Singa
 - Caffe
 - H2O
 - PyTorch
 - mlpack
 - TensorFlow
 - Torch
 - CNTK
 - Accord.Net</p>

<h3 id="5-2-machine-learning-libraries">5.2 Machine learning libraries</h3>

<ul>
<li>Deeplearning4j</li>
<li>Theano</li>
<li>Scikit-learn</li>
</ul>

<h3 id="5-3-machine-learning-algorithms">5.3 Machine learning algorithms</h3>

<ul>
<li>Almeida–Pineda recurrent backpropagation</li>
<li>ALOPEX</li>
<li>Backpropagation</li>
<li>Bootstrap aggregating</li>
<li>CN2 algorithm</li>
<li>Constructing skill trees</li>
<li>Dehaene–Changeux model</li>
<li>Diffusion map</li>
<li>Dominance-based rough set approach</li>
<li>Dynamic time warping</li>
<li>Error-driven learning</li>
<li>Evolutionary multimodal optimization</li>
<li>Expectation–maximization algorithm</li>
<li>FastICA</li>
<li>Forward–backward algorithm</li>
<li>GeneRec</li>
<li>Genetic Algorithm for Rule Set Production</li>
<li>Growing self-organizing map</li>
<li>HEXQ</li>
<li>Hyper basis function network</li>
<li>IDistance</li>
<li>K-nearest neighbors algorithm</li>
<li>Kernel methods for vector output</li>
<li>Kernel principal component analysis</li>
<li>Leabra</li>
<li>Linde–Buzo–Gray algorithm</li>
<li>Local outlier factor</li>
<li>Logic learning machine</li>
<li>LogitBoost</li>
<li>Manifold alignment</li>
<li>Minimum redundancy feature selection</li>
<li>Mixture of experts</li>
<li>Multiple kernel learning</li>
<li>Non-negative matrix factorization</li>
<li>Online machine learning</li>
<li>Out-of-bag error</li>
<li>Prefrontal cortex basal ganglia working memory</li>
<li>PVLV</li>
<li>Q-learning</li>
<li>Quadratic unconstrained binary optimization</li>
<li>Query-level feature</li>
<li>Quickprop</li>
<li>Radial basis function network</li>
<li>Randomized weighted majority algorithm</li>
<li>Reinforcement learning</li>
<li>Repeated incremental pruning to produce error reduction (RIPPER)</li>
<li>Rprop</li>
<li>Rule-based machine learning</li>
<li>Skill chaining</li>
<li>Sparse PCA</li>
<li>State–action–reward–state–action</li>
<li>Stochastic gradient descent</li>
<li>Structured kNN</li>
<li>T-distributed stochastic neighbor embedding</li>
<li>Temporal difference learning</li>
<li>Wake-sleep algorithm</li>
<li>Weighted majority algorithm (machine learning)</li>
</ul>

<h2 id="6-machine-learning-methods">6 Machine learning methods</h2>

<ul>
<li>Instance-based algorithm

<ul>
<li>K-nearest neighbors algorithm (KNN)</li>
<li>Learning vector quantization (LVQ)</li>
<li>Self-organizing map (SOM)</li>
</ul></li>
<li>Regression analysis

<ul>
<li>Logistic regression</li>
<li>Ordinary least squares regression (OLSR)</li>
<li>Linear regression</li>
<li>Stepwise regression</li>
<li>Multivariate adaptive regression splines (MARS)</li>
</ul></li>
<li>Regularization algorithm

<ul>
<li>Ridge regression</li>
<li>Least Absolute Shrinkage and Selection Operator (LASSO)</li>
<li>Elastic net</li>
<li>Least-angle regression (LARS)</li>
</ul></li>
<li>Classifiers

<ul>
<li>Probabilistic classifier

<ul>
<li>Naive Bayes classifier</li>
</ul></li>
<li>Binary classifier</li>
<li>Linear classifier</li>
<li>Hierarchical classifier</li>
</ul></li>
</ul>

<h3 id="6-1-dimensionality-reduction">6.1 Dimensionality reduction</h3>

<ul>
<li>Canonical correlation analysis (CCA)</li>
<li>Factor analysis</li>
<li>Feature extraction</li>
<li>Feature selection</li>
<li>Independent component analysis (ICA)</li>
<li>Linear discriminant analysis (LDA)</li>
<li>Multidimensional scaling (MDS)</li>
<li>Non-negative matrix factorization (NMF)</li>
<li>Partial least squares regression (PLSR)</li>
<li>Principal component analysis (PCA)</li>
<li>Principal component regression (PCR)</li>
<li>Projection pursuit</li>
<li>Sammon mapping</li>
<li>t-distributed stochastic neighbor embedding (t-SNE)</li>
</ul>

<h3 id="6-2-ensemble-learning">6.2 Ensemble learning</h3>

<ul>
<li>AdaBoost</li>
<li>Boosting</li>
<li>Bootstrap aggregating (Bagging)</li>
<li>Ensemble averaging – process of creating multiple models and combining them to produce a desired output, as opposed to creating just one model. Frequently an ensemble of models performs better than any individual model, because the various errors of the models &quot;average out.&quot;</li>
<li>Gradient boosted decision tree (GBDT)</li>
<li>Gradient boosting machine (GBM)</li>
<li>Random Forest</li>
<li>Stacked Generalization (blending)</li>
</ul>

<h3 id="6-3-meta-learning">6.3 Meta learning</h3>

<ul>
<li>Inductive bias</li>
<li>Metadata</li>
</ul>

<h3 id="6-4-reinforcement-learning">6.4 Reinforcement learning</h3>

<ul>
<li>Q-learning</li>
<li>State–action–reward–state–action (SARSA)</li>
<li>Temporal difference learning (TD)</li>
<li>Learning Automata</li>
</ul>

<h3 id="6-5-supervised-learning">6.5 Supervised learning</h3>

<p><strong>Supervised learning</strong>
 - AODE
 - Artificial neural network
 - Association rule learning algorithms
   - Apriori algorithm
   - Eclat algorithm
 - Case-based reasoning
 - Gaussian process regression
 - Gene expression programming
 - Group method of data handling (GMDH)
 - Inductive logic programming
 - Instance-based learning
 - Lazy learning
 - Learning Automata
 - Learning Vector Quantization
 - Logistic Model Tree
 - Minimum message length (decision trees, decision graphs, etc.)
   - Nearest Neighbor Algorithm
   - Analogical modeling
 - Probably approximately correct learning (PAC) learning
 - Ripple down rules, a knowledge acquisition methodology
 - Symbolic machine learning algorithms
 - Support vector machines
 - Random Forests
 - Ensembles of classifiers
   - Bootstrap aggregating (bagging)
   - Boosting (meta-algorithm)
 - Ordinal classification
 - Information fuzzy networks (IFN)
 - Conditional Random Field
 - ANOVA
 - Quadratic classifiers
 - k-nearest neighbor
 - Boosting
   - SPRINT
 - Bayesian networks
   - Naive Bayes
 - Hidden Markov models
   - Hierarchical hidden Markov model
<strong>Bayesian statistics</strong>
 - Bayesian knowledge base
 - Naive Bayes
 - Gaussian Naive Bayes
 - Multinomial Naive Bayes
 - Averaged One-Dependence Estimators (AODE)
 - Bayesian Belief Network (BBN)
 - Bayesian Network (BN)
<strong>Decision tree algorithms</strong>
 - Decision tree
 - Classification and regression tree (CART)
 - Iterative Dichotomiser 3 (ID3)
 - C4.5 algorithm
 - C5.0 algorithm
 - Chi-squared Automatic Interaction Detection (CHAID)
 - Decision stump
 - Conditional decision tree
 - ID3 algorithm
 - Random forest
 - SLIQ
<strong>Linear classifier</strong>
 - Fisher's linear discriminant
 - Linear regression
 - Logistic regression
 - Multinomial logistic regression
 - Naive Bayes classifier
 - Perceptron
 - Support vector machine</p>

<h3 id="6-6-unsupervised-learning">6.6 Unsupervised learning</h3>

<p><strong>Unsupervised learning</strong>
 - Expectation-maximization algorithm
 - Vector Quantization
 - Generative topographic map
 - Information bottleneck method
<strong>Artificial neural networks</strong>
 - Feedforward neural network
   - Extreme learning machine
   - Convolutional neural network
 - Recurrent neural network
   - Long short-term memory (LSTM)
 - Logic learning machine
 - Self-organizing map
<strong>Association rule learning</strong>
 - Apriori algorithm
 - Eclat algorithm
 - FP-growth algorithm
<strong>Hierarchical clustering</strong>
 - Single-linkage clustering
 - Conceptual clustering
<strong>Cluster analysis</strong>
 - BIRCH
 - DBSCAN
 - Expectation-maximization (EM)
 - Fuzzy clustering
 - Hierarchical Clustering
 - K-means clustering
 - K-medians
 - Mean-shift
 - OPTICS algorithm
<strong>Anomaly detection</strong>
 - k-nearest neighbors classification (k-NN)
 - Local outlier factor</p>

<h3 id="6-7-semi-supervised-learning">6.7 Semi-supervised learning</h3>

<ul>
<li>Active learning – special case of semi-supervised learning in which a learning algorithm is able to interactively query the user (or some other information source) to obtain the desired outputs at new data points.[5] [6]</li>
<li>Generative models</li>
<li>Low-density separation</li>
<li>Graph-based methods</li>
<li>Co-training</li>
<li>Transduction</li>
</ul>

<h3 id="6-8-deep-learning">6.8 Deep learning</h3>

<ul>
<li>Deep belief networks</li>
<li>Deep Convolutional neural networks</li>
<li>Deep Recurrent neural networks</li>
<li>Hierarchical temporal memory</li>
<li>Generative Adversarial Networks</li>
<li>Deep Boltzmann Machine (DBM)</li>
<li>Stacked Auto-Encoders</li>
</ul>

<h3 id="6-9-other-machine-learning-methods-and-problems">6.9 Other machine learning methods and problems</h3>

<ul>
<li>Anomaly detection</li>
<li>Association rules</li>
<li>Bias-variance dilemma</li>
<li>Classification

<ul>
<li>Multi-label classification</li>
</ul></li>
<li>Clustering</li>
<li>Data Pre-processing</li>
<li>Empirical risk minimization</li>
<li>Feature engineering</li>
<li>Feature learning</li>
<li>Learning to rank</li>
<li>Occam learning</li>
<li>Online machine learning</li>
<li>PAC learning</li>
<li>Regression</li>
<li>Reinforcement Learning</li>
<li>Semi-supervised learning</li>
<li>Statistical learning</li>
<li>Structured prediction

<ul>
<li>Graphical models

<ul>
<li>Bayesian network</li>
<li>Conditional random field (CRF)</li>
<li>Hidden Markov model (HMM)</li>
</ul></li>
</ul></li>
<li>Unsupervised learning</li>
<li>VC theory</li>
</ul>

<h2 id="7-machine-learning-research">7 Machine learning research</h2>

<ul>
<li>List of artificial intelligence projects</li>
<li>List of datasets for machine learning research</li>
</ul>

<h2 id="8-history-of-machine-learning">8 History of machine learning</h2>

<ul>
<li>Timeline of machine learning</li>
</ul>

<h2 id="9-machine-learning-projects">9 Machine learning projects</h2>

<ul>
<li>DeepMind</li>
<li>Google Brain</li>
</ul>

<h2 id="10-machine-learning-organizations">10 Machine learning organizations</h2>

<ul>
<li>Knowledge Engineering and Machine Learning Group</li>
</ul>

<h3 id="10-1-machine-learning-conferences-and-workshops">10.1 Machine learning conferences and workshops</h3>

<ul>
<li>Artificial Intelligence and Security (AISec) (co-located workshop with CCS)</li>
<li>Conference on Neural Information Processing Systems (NIPS)</li>
<li>ECML PKDD</li>
<li>International Conference on Machine Learning (ICML)</li>
</ul>

<h2 id="11-machine-learning-publications">11 Machine learning publications</h2>

<h3 id="11-1-books-on-machine-learning">11.1 Books on machine learning</h3>

<h3 id="11-2-machine-learning-journals">11.2 Machine learning journals</h3>

<ul>
<li>Machine Learning</li>
<li>Journal of Machine Learning Research (JMLR)</li>
<li>Neural Computation</li>
</ul>

<h2 id="12-persons-influential-in-machine-learning">12 Persons influential in machine learning</h2>

<ul>
<li>Alberto Broggi</li>
<li>Andrei Knyazev</li>
<li>Andrew McCallum</li>
<li>Andrew Ng</li>
<li>Anuraag Jain</li>
<li>Armin B. Cremers</li>
<li>Ayanna Howard</li>
<li>Barney Pell</li>
<li>Ben Goertzel</li>
<li>Ben Taskar</li>
<li>Bernhard Schölkopf</li>
<li>Brian D. Ripley</li>
<li>Christopher G. Atkeson</li>
<li>Corinna Cortes</li>
<li>Demis Hassabis</li>
<li>Douglas Lenat</li>
<li>Eric Xing</li>
<li>Ernst Dickmanns</li>
<li>Geoffrey Hinton – co-inventor of the backpropagation and contrastive divergence training algorithms</li>
<li>Hans-Peter Kriegel</li>
<li>Hartmut Neven</li>
<li>Heikki Mannila</li>
<li>Ian Goodfellow – Father of Generative &amp; adversarial networks [7]</li>
<li>Jacek M. Zurada</li>
<li>Jaime Carbonell</li>
<li>Jeremy Slovak</li>
<li>Jerome H. Friedman</li>
<li>John D. Lafferty</li>
<li>John Platt – invented SMO and Platt scaling</li>
<li>Julie Beth Lovins</li>
<li>Jürgen Schmidhuber</li>
<li>Karl Steinbuch</li>
<li>Katia Sycara</li>
<li>Leo Breiman – invented bagging and random forests</li>
<li>Lise Getoor</li>
<li>Luca Maria Gambardella</li>
<li>Léon Bottou</li>
<li>Marcus Hutter</li>
<li>Mehryar Mohri</li>
<li>Michael Collins</li>
<li>Michael I. Jordan</li>
<li>Michael L. Littman</li>
<li>Nando de Freitas</li>
<li>Ofer Dekel</li>
<li>Oren Etzioni</li>
<li>Pedro Domingos</li>
<li>Peter Flach</li>
<li>Pierre Baldi</li>
<li>Pushmeet Kohli</li>
<li>Ray Kurzweil</li>
<li>Rayid Ghani</li>
<li>Ross Quinlan</li>
<li>Salvatore J. Stolfo</li>
<li>Sebastian Thrun</li>
<li>Selmer Bringsjord</li>
<li>Sepp Hochreiter</li>
<li>Shane Legg</li>
<li>Siraj Raval</li>
<li>Stephen Muggleton</li>
<li>Steve Omohundro</li>
<li>Tom M. Mitchell</li>
<li>Trevor Hastie</li>
<li>Vasant Honavar</li>
<li>Vladimir Vapnik – co-inventor of the SVM and VC theory</li>
<li>Yann LeCun – invented convolutional neural networks</li>
<li>Yasuo Matsuyama</li>
<li>Yoshua Bengio</li>
<li>Zoubin Ghahramani</li>
</ul>

<h2 id="13-see-also">13 See also</h2>

<ul>
<li>Outline of artificial intelligence

<ul>
<li>Outline of computer vision</li>
<li>Outline of natural language processing</li>
</ul></li>
<li>Outline of robotics</li>
<li>Accuracy paradox</li>
<li>Action model learning</li>
<li>Activation function</li>
<li>Activity recognition</li>
<li>ADALINE</li>
<li>Adaptive neuro fuzzy inference system</li>
<li>Adaptive resonance theory</li>
<li>Additive smoothing</li>
<li>Adjusted mutual information</li>
<li>Aika (software)</li>
<li>AIVA</li>
<li>AIXI</li>
<li>AlchemyAPI</li>
<li>AlexNet</li>
<li>Algorithm selection</li>
<li>Algorithmic inference</li>
<li>Algorithmic learning theory</li>
<li>AlphaGo</li>
<li>AlphaGo Zero</li>
<li>Alternating decision tree</li>
<li>Apprenticeship learning</li>
<li>Causal Markov condition</li>
<li>Competitive learning</li>
<li>Concept learning</li>
<li>Decision tree learning</li>
<li>Distribution learning theory</li>
<li>Eager learning</li>
<li>End-to-end reinforcement learning</li>
<li>Error tolerance (PAC learning)</li>
<li>Explanation-based learning</li>
<li>Feature</li>
<li>GloVe</li>
<li>Hyperparameter</li>
<li>IBM Machine Learning Hub</li>
<li>Inferential theory of learning</li>
<li>Learning automata</li>
<li>Learning classifier system</li>
<li>Learning rule</li>
<li>Learning with errors</li>
<li>M-Theory (learning framework)</li>
<li>Machine learning control</li>
<li>Machine learning in bioinformatics</li>
<li>Margin</li>
<li>Markov chain geostatistics</li>
<li>Markov chain Monte Carlo (MCMC)</li>
<li>Markov information source</li>
<li>Markov logic network</li>
<li>Markov model</li>
<li>Markov random field</li>
<li>Markovian discrimination</li>
<li>Maximum-entropy Markov model</li>
<li>Multi-armed bandit</li>
<li>Multi-task learning</li>
<li>Multilinear subspace learning</li>
<li>Multimodal learning</li>
<li>Multiple instance learning</li>
<li>Multiple-instance learning</li>
<li>Never-Ending Language Learning</li>
<li>Offline learning</li>
<li>Parity learning</li>
<li>Population-based incremental learning</li>
<li>Predictive learning</li>
<li>Preference learning</li>
<li>Proactive learning</li>
<li>Proximal gradient methods for learning</li>
<li>Semantic analysis</li>
<li>Similarity learning</li>
<li>Sparse dictionary learning</li>
<li>Stability (learning theory)</li>
<li>Statistical learning theory</li>
<li>Statistical relational learning</li>
<li>Tanagra</li>
<li>Transfer learning</li>
<li>Variable-order Markov model</li>
<li>Version space learning</li>
<li>Waffles</li>
<li>Weka</li>
<li>Loss function

<ul>
<li>Loss functions for classification</li>
<li>Mean squared error (MSE)</li>
<li>Mean squared prediction error (MSPE)</li>
<li>Taguchi loss function</li>
</ul></li>
<li>Low-energy adaptive clustering hierarchy</li>
</ul>

<h3 id="13-1-other">13.1 Other</h3>

<ul>
<li>Anne O'Tate</li>
<li>Ant colony optimization algorithms</li>
<li>Anthony Levandowski</li>
<li>Anti-unification (computer science)</li>
<li>Apache Flume</li>
<li>Apache Giraph</li>
<li>Apache Mahout</li>
<li>Apache SINGA</li>
<li>Apache Spark</li>
<li>Apache SystemML</li>
<li>Aphelion (software)</li>
<li>Arabic Speech Corpus</li>
<li>Archetypal analysis</li>
<li>Arthur Zimek</li>
<li>Artificial ants</li>
<li>Artificial bee colony algorithm</li>
<li>Artificial development</li>
<li>Artificial immune system</li>
<li>Astrostatistics</li>
<li>Averaged one-dependence estimators</li>
<li>Bag-of-words model</li>
<li>Balanced clustering</li>
<li>Ball tree</li>
<li>Base rate</li>
<li>Bat algorithm</li>
<li>Baum–Welch algorithm</li>
<li>Bayesian hierarchical modeling</li>
<li>Bayesian interpretation of kernel regularization</li>
<li>Bayesian optimization</li>
<li>Bayesian structural time series</li>
<li>Bees algorithm</li>
<li>Behavioral clustering</li>
<li>Bernoulli scheme</li>
<li>Bias–variance tradeoff</li>
<li>Biclustering</li>
<li>Binarization of consensus partition matrices</li>
<li>Binary classification</li>
<li>Bing Predicts</li>
<li>Bio-inspired computing</li>
<li>Biogeography-based optimization</li>
<li>Biplot</li>
<li>Bondy's theorem</li>
<li>Bongard problem</li>
<li>Bradley–Terry model</li>
<li>BrownBoost</li>
<li>Brown clustering</li>
<li>Burst error</li>
<li>CBCL (MIT)</li>
<li>CIML community portal</li>
<li>CMA-ES</li>
<li>CURE data clustering algorithm</li>
<li>Cache language model</li>
<li>Calibration (statistics)</li>
<li>Canonical correspondence analysis</li>
<li>Canopy clustering algorithm</li>
<li>Cascading classifiers</li>
<li>Category utility</li>
<li>CellCognition</li>
<li>Cellular evolutionary algorithm</li>
<li>Chi-square automatic interaction detection</li>
<li>Chromosome (genetic algorithm)</li>
<li>Classifier chains</li>
<li>Cleverbot</li>
<li>Clonal selection algorithm</li>
<li>Cluster-weighted modeling</li>
<li>Clustering high-dimensional data</li>
<li>Clustering illusion</li>
<li>CoBoosting</li>
<li>Cobweb (clustering)</li>
<li>Cognitive computer</li>
<li>Cognitive robotics</li>
<li>Collostructional analysis</li>
<li>Common-method variance</li>
<li>Complete-linkage clustering</li>
<li>Computer-automated design</li>
<li>Concept class</li>
<li>Concept drift</li>
<li>Conference on Artificial General Intelligence</li>
<li>Conference on Knowledge Discovery and Data Mining</li>
<li>Confirmatory factor analysis</li>
<li>Confusion matrix</li>
<li>Congruence coefficient</li>
<li>Connect (computer system)</li>
<li>Consensus clustering</li>
<li>Constrained clustering</li>
<li>Constrained conditional model</li>
<li>Constructive cooperative coevolution</li>
<li>Correlation clustering</li>
<li>Correspondence analysis</li>
<li>Cortica</li>
<li>Coupled pattern learner</li>
<li>Cross-entropy method</li>
<li>Cross-validation (statistics)</li>
<li>Crossover (genetic algorithm)</li>
<li>Cuckoo search</li>
<li>Cultural algorithm</li>
<li>Cultural consensus theory</li>
<li>Curse of dimensionality</li>
<li>DADiSP</li>
<li>DARPA LAGR Program</li>
<li>Darkforest</li>
<li>Dartmouth workshop</li>
<li>DarwinTunes</li>
<li>Data Mining Extensions</li>
<li>Data exploration</li>
<li>Data pre-processing</li>
<li>Data stream clustering</li>
<li>Dataiku</li>
<li>Davies–Bouldin index</li>
<li>Decision boundary</li>
<li>Decision list</li>
<li>Decision tree model</li>
<li>Deductive classifier</li>
<li>DeepArt</li>
<li>DeepDream</li>
<li>Deep Web Technologies</li>
<li>Defining length</li>
<li>Dendrogram</li>
<li>Dependability state model</li>
<li>Detailed balance</li>
<li>Determining the number of clusters in a data set</li>
<li>Detrended correspondence analysis</li>
<li>Developmental robotics</li>
<li>Diffbot</li>
<li>Differential evolution</li>
<li>Discrete phase-type distribution</li>
<li>Discriminative model</li>
<li>Dissociated press</li>
<li>Distributed R</li>
<li>Dlib</li>
<li>Document classification</li>
<li>Documenting Hate</li>
<li>Domain adaptation</li>
<li>Doubly stochastic model</li>
<li>Dual-phase evolution</li>
<li>Dunn index</li>
<li>Dynamic Bayesian network</li>
<li>Dynamic Markov compression</li>
<li>Dynamic topic model</li>
<li>Dynamic unobserved effects model</li>
<li>EDLUT</li>
<li>ELKI</li>
<li>Edge recombination operator</li>
<li>Effective fitness</li>
<li>Elastic map</li>
<li>Elastic matching</li>
<li>Elbow method (clustering)</li>
<li>Emergent (software)</li>
<li>Encog</li>
<li>Entropy rate</li>
<li>Erkki Oja</li>
<li>Eurisko</li>
<li>European Conference on Artificial Intelligence</li>
<li>Evaluation of binary classifiers</li>
<li>Evolution strategy</li>
<li>Evolution window</li>
<li>Evolutionary Algorithm for Landmark Detection</li>
<li>Evolutionary algorithm</li>
<li>Evolutionary art</li>
<li>Evolutionary music</li>
<li>Evolutionary programming</li>
<li>Evolvability (computer science)</li>
<li>Evolved antenna</li>
<li>Evolver (software)</li>
<li>Evolving classification function</li>
<li>Expectation propagation</li>
<li>Exploratory factor analysis</li>
<li>F1 score</li>
<li>FLAME clustering</li>
<li>Factor analysis of mixed data</li>
<li>Factor graph</li>
<li>Factor regression model</li>
<li>Factored language model</li>
<li>Farthest-first traversal</li>
<li>Fast-and-frugal trees</li>
<li>Feature Selection Toolbox</li>
<li>Feature hashing</li>
<li>Feature scaling</li>
<li>Feature vector</li>
<li>Firefly algorithm</li>
<li>First-difference estimator</li>
<li>First-order inductive learner</li>
<li>Fish School Search</li>
<li>Fisher kernel</li>
<li>Fitness approximation</li>
<li>Fitness function</li>
<li>Fitness proportionate selection</li>
<li>Fluentd</li>
<li>Folding@home</li>
<li>Formal concept analysis</li>
<li>Forward algorithm</li>
<li>Fowlkes–Mallows index</li>
<li>Frederick Jelinek</li>
<li>Frrole</li>
<li>Functional principal component analysis</li>
<li>GATTO</li>
<li>GLIMMER</li>
<li>Gary Bryce Fogel</li>
<li>Gaussian adaptation</li>
<li>Gaussian process</li>
<li>Gaussian process emulator</li>
<li>Gene prediction</li>
<li>General Architecture for Text Engineering</li>
<li>Generalization error</li>
<li>Generalized canonical correlation</li>
<li>Generalized filtering</li>
<li>Generalized iterative scaling</li>
<li>Generalized multidimensional scaling</li>
<li>Generative adversarial network</li>
<li>Generative model</li>
<li>Genetic algorithm</li>
<li>Genetic algorithm scheduling</li>
<li>Genetic algorithms in economics</li>
<li>Genetic fuzzy systems</li>
<li>Genetic memory (computer science)</li>
<li>Genetic operator</li>
<li>Genetic programming</li>
<li>Genetic representation</li>
<li>Geographical cluster</li>
<li>Gesture Description Language</li>
<li>Geworkbench</li>
<li>Glossary of artificial intelligence</li>
<li>Glottochronology</li>
<li>Golem (ILP)</li>
<li>Google matrix</li>
<li>Grafting (decision trees)</li>
<li>Gramian matrix</li>
<li>Grammatical evolution</li>
<li>Granular computing</li>
<li>GraphLab</li>
<li>Graph kernel</li>
<li>Gremlin (programming language)</li>
<li>Growth function</li>
<li>HUMANT (HUManoid ANT) algorithm</li>
<li>Hammersley–Clifford theorem</li>
<li>Harmony search</li>
<li>Hebbian theory</li>
<li>Hidden Markov random field</li>
<li>Hidden semi-Markov model</li>
<li>Hierarchical hidden Markov model</li>
<li>Higher-order factor analysis</li>
<li>Highway network</li>
<li>Hinge loss</li>
<li>Holland's schema theorem</li>
<li>Hopkins statistic</li>
<li>Hoshen–Kopelman algorithm</li>
<li>Huber loss</li>
<li>IRCF360</li>
<li>Ian Goodfellow</li>
<li>Ilastik</li>
<li>Ilya Sutskever</li>
<li>Immunocomputing</li>
<li>Imperialist competitive algorithm</li>
<li>Inauthentic text</li>
<li>Incremental decision tree</li>
<li>Induction of regular languages</li>
<li>Inductive bias</li>
<li>Inductive probability</li>
<li>Inductive programming</li>
<li>Influence diagram</li>
<li>Information Harvesting</li>
<li>Information fuzzy networks</li>
<li>Information gain in decision trees</li>
<li>Information gain ratio</li>
<li>Inheritance (genetic algorithm)</li>
<li>Instance selection</li>
<li>Intel RealSense</li>
<li>Interacting particle system</li>
<li>Interactive machine translation</li>
<li>International Joint Conference on Artificial Intelligence</li>
<li>International Meeting on Computational Intelligence Methods for Bioinformatics and Biostatistics</li>
<li>International Semantic Web Conference</li>
<li>Iris flower data set</li>
<li>Island algorithm</li>
<li>Isotropic position</li>
<li>Item response theory</li>
<li>Iterative Viterbi decoding</li>
<li>JOONE</li>
<li>Jabberwacky</li>
<li>Jaccard index</li>
<li>Jackknife variance estimates for random forest</li>
<li>Java Grammatical Evolution</li>
<li>Joseph Nechvatal</li>
<li>Jubatus</li>
<li>Julia (programming language)</li>
<li>Junction tree algorithm</li>
<li>K-SVD</li>
<li>K-means++</li>
<li>K-medians clustering</li>
<li>K-medoids</li>
<li>KNIME</li>
<li>KXEN Inc.</li>
<li>K q-flats</li>
<li>Kaggle</li>
<li>Kalman filter</li>
<li>Katz's back-off model</li>
<li>Keras</li>
<li>Kernel adaptive filter</li>
<li>Kernel density estimation</li>
<li>Kernel eigenvoice</li>
<li>Kernel embedding of distributions</li>
<li>Kernel method</li>
<li>Kernel perceptron</li>
<li>Kernel random forest</li>
<li>Kinect</li>
<li>Klaus-Robert Müller</li>
<li>Kneser–Ney smoothing</li>
<li>Knowledge Vault</li>
<li>Knowledge integration</li>
<li>LIBSVM</li>
<li>LPBoost</li>
<li>Labeled data</li>
<li>LanguageWare</li>
<li>Language Acquisition Device (computer)</li>
<li>Language identification in the limit</li>
<li>Language model</li>
<li>Large margin nearest neighbor</li>
<li>Latent Dirichlet allocation</li>
<li>Latent class model</li>
<li>Latent semantic analysis</li>
<li>Latent variable</li>
<li>Latent variable model</li>
<li>Lattice Miner</li>
<li>Layered hidden Markov model</li>
<li>Learnable function class</li>
<li>Least squares support vector machine</li>
<li>Leave-one-out error</li>
<li>Leslie P. Kaelbling</li>
<li>Linear genetic programming</li>
<li>Linear predictor function</li>
<li>Linear separability</li>
<li>Lingyun Gu</li>
<li>Linkurious</li>
<li>Lior Ron (business executive)</li>
<li>List of genetic algorithm applications</li>
<li>List of metaphor-based metaheuristics</li>
<li>List of text mining software</li>
<li>Local case-control sampling</li>
<li>Local independence</li>
<li>Local tangent space alignment</li>
<li>Locality-sensitive hashing</li>
<li>Log-linear model</li>
<li>Logistic model tree</li>
<li>Low-rank approximation</li>
<li>Low-rank matrix approximations</li>
<li>MATLAB</li>
<li>MIMIC (immunology)</li>
<li>MXNet</li>
<li>Mallet (software project)</li>
<li>Manifold regularization</li>
<li>Margin-infused relaxed algorithm</li>
<li>Margin classifier</li>
<li>Mark V. Shaney</li>
<li>Massive Online Analysis</li>
<li>Matrix regularization</li>
<li>Matthews correlation coefficient</li>
<li>Mean shift</li>
<li>Mean squared error</li>
<li>Mean squared prediction error</li>
<li>Measurement invariance</li>
<li>Medoid</li>
<li>MeeMix</li>
<li>Melomics</li>
<li>Memetic algorithm</li>
<li>Meta-optimization</li>
<li>Mexican International Conference on Artificial Intelligence</li>
<li>Michael Kearns (computer scientist)</li>
<li>MinHash</li>
<li>Mixture model</li>
<li>Mlpy</li>
<li>Models of DNA evolution</li>
<li>Moral graph</li>
<li>Mountain car problem</li>
<li>Movidius</li>
<li>Multi-armed bandit</li>
<li>Multi-label classification</li>
<li>Multi expression programming</li>
<li>Multiclass classification</li>
<li>Multidimensional analysis</li>
<li>Multifactor dimensionality reduction</li>
<li>Multilinear principal component analysis</li>
<li>Multiple correspondence analysis</li>
<li>Multiple discriminant analysis</li>
<li>Multiple factor analysis</li>
<li>Multiple sequence alignment</li>
<li>Multiplicative weight update method</li>
<li>Multispectral pattern recognition</li>
<li>Mutation (genetic algorithm)</li>
<li>MysteryVibe</li>
<li>N-gram</li>
<li>NOMINATE (scaling method)</li>
<li>Native-language identification</li>
<li>Natural Language Toolkit</li>
<li>Natural evolution strategy</li>
<li>Nearest-neighbor chain algorithm</li>
<li>Nearest centroid classifier</li>
<li>Nearest neighbor search</li>
<li>Neighbor joining</li>
<li>Nest Labs</li>
<li>NetMiner</li>
<li>NetOwl</li>
<li>Neural Designer</li>
<li>Neural Engineering Object</li>
<li>Neural Lab</li>
<li>Neural modeling fields</li>
<li>Neural network software</li>
<li>NeuroSolutions</li>
<li>Neuro Laboratory</li>
<li>Neuroevolution</li>
<li>Neuroph</li>
<li>Niki.ai</li>
<li>Noisy channel model</li>
<li>Noisy text analytics</li>
<li>Nonlinear dimensionality reduction</li>
<li>Novelty detection</li>
<li>Nuisance variable</li>
<li>Numenta</li>
<li>One-class classification</li>
<li>Onnx</li>
<li>OpenNLP</li>
<li>Optimal discriminant analysis</li>
<li>Oracle Data Mining</li>
<li>Orange (software)</li>
<li>Ordination (statistics)</li>
<li>Overfitting</li>
<li>PROGOL</li>
<li>PSIPRED</li>
<li>Pachinko allocation</li>
<li>PageRank</li>
<li>Parallel metaheuristic</li>
<li>Parity benchmark</li>
<li>Part-of-speech tagging</li>
<li>Particle swarm optimization</li>
<li>Path dependence</li>
<li>Pattern language (formal languages)</li>
<li>Peltarion Synapse</li>
<li>Perplexity</li>
<li>Persian Speech Corpus</li>
<li>Picas (app)</li>
<li>Pietro Perona</li>
<li>Pipeline Pilot</li>
<li>Piranha (software)</li>
<li>Pitman–Yor process</li>
<li>Plate notation</li>
<li>Polynomial kernel</li>
<li>Pop music automation</li>
<li>Population process</li>
<li>Portable Format for Analytics</li>
<li>Predictive Model Markup Language</li>
<li>Predictive state representation</li>
<li>Preference regression</li>
<li>Premature convergence</li>
<li>Principal geodesic analysis</li>
<li>Prior knowledge for pattern recognition</li>
<li>Prisma (app)</li>
<li>Probabilistic Action Cores</li>
<li>Probabilistic context-free grammar</li>
<li>Probabilistic latent semantic analysis</li>
<li>Probabilistic soft logic</li>
<li>Probability matching</li>
<li>Probit model</li>
<li>Product of experts</li>
<li>Programming with Big Data in R</li>
<li>Proper generalized decomposition</li>
<li>Pruning (decision trees)</li>
<li>Pushpak Bhattacharyya</li>
<li>Q methodology</li>
<li>Qloo</li>
<li>Quality control and genetic algorithms</li>
<li>Quantum Artificial Intelligence Lab</li>
<li>Queueing theory</li>
<li>Quick, Draw!</li>
<li>R (programming language)</li>
<li>Rada Mihalcea</li>
<li>Rademacher complexity</li>
<li>Radial basis function kernel</li>
<li>Rand index</li>
<li>Random indexing</li>
<li>Random projection</li>
<li>Random subspace method</li>
<li>Ranking SVM</li>
<li>RapidMiner</li>
<li>Rattle GUI</li>
<li>Raymond Cattell</li>
<li>Reasoning system</li>
<li>Regularization perspectives on support vector machines</li>
<li>Relational data mining</li>
<li>Relationship square</li>
<li>Relevance vector machine</li>
<li>Relief (feature selection)</li>
<li>Renjin</li>
<li>Repertory grid</li>
<li>Representer theorem</li>
<li>Reward-based selection</li>
<li>Richard Zemel</li>
<li>Right to explanation</li>
<li>RoboEarth</li>
<li>Robust principal component analysis</li>
<li>RuleML Symposium</li>
<li>Rule induction</li>
<li>Rules extraction system family</li>
<li>SAS (software)</li>
<li>SNNS</li>
<li>SPSS Modeler</li>
<li>SUBCLU</li>
<li>Sample complexity</li>
<li>Sample exclusion dimension</li>
<li>Santa Fe Trail problem</li>
<li>Savi Technology</li>
<li>Schema (genetic algorithms)</li>
<li>Search-based software engineering</li>
<li>Selection (genetic algorithm)</li>
<li>Self-Service Semantic Suite</li>
<li>Semantic folding</li>
<li>Semantic mapping (statistics)</li>
<li>Semidefinite embedding</li>
<li>Sense Networks</li>
<li>Sensorium Project</li>
<li>Sequence labeling</li>
<li>Sequential minimal optimization</li>
<li>Shattered set</li>
<li>Shogun (toolbox)</li>
<li>Silhouette (clustering)</li>
<li>SimHash</li>
<li>SimRank</li>
<li>Similarity measure</li>
<li>Simple matching coefficient</li>
<li>Simultaneous localization and mapping</li>
<li>Sinkov statistic</li>
<li>Sliced inverse regression</li>
<li>SmartMatch</li>
<li>Snakes and Ladders</li>
<li>Soft independent modelling of class analogies</li>
<li>Soft output Viterbi algorithm</li>
<li>Solomonoff's theory of inductive inference</li>
<li>SolveIT Software</li>
<li>Spectral clustering</li>
<li>Spike-and-slab variable selection</li>
<li>Statistical machine translation</li>
<li>Statistical parsing</li>
<li>Statistical semantics</li>
<li>Stefano Soatto</li>
<li>Stephen Wolfram</li>
<li>Stochastic block model</li>
<li>Stochastic cellular automaton</li>
<li>Stochastic diffusion search</li>
<li>Stochastic grammar</li>
<li>Stochastic matrix</li>
<li>Stochastic universal sampling</li>
<li>Stress majorization</li>
<li>String kernel</li>
<li>Structural equation modeling</li>
<li>Structural risk minimization</li>
<li>Structured sparsity regularization</li>
<li>Structured support vector machine</li>
<li>Subclass reachability</li>
<li>Sufficient dimension reduction</li>
<li>Sukhotin's algorithm</li>
<li>Sum of absolute differences</li>
<li>Sum of absolute transformed differences</li>
<li>Swarm intelligence</li>
<li>Switching Kalman filter</li>
<li>Symbolic regression</li>
<li>Synchronous context-free grammar</li>
<li>Syntactic pattern recognition</li>
<li>TD-Gammon</li>
<li>TIMIT</li>
<li>Teaching dimension</li>
<li>Teuvo Kohonen</li>
<li>Textual case-based reasoning</li>
<li>Theory of conjoint measurement</li>
<li>Thomas G. Dietterich</li>
<li>Thurstonian model</li>
<li>Topic model</li>
<li>Tournament selection</li>
<li>Training, test, and validation sets</li>
<li>Transiogram</li>
<li>Trax Image Recognition</li>
<li>Trigram tagger</li>
<li>Truncation selection</li>
<li>Tucker decomposition</li>
<li>UIMA</li>
<li>UPGMA</li>
<li>Ugly duckling theorem</li>
<li>Uncertain data</li>
<li>Uniform convergence in probability</li>
<li>Unique negative dimension</li>
<li>Universal portfolio algorithm</li>
<li>User behavior analytics</li>
<li>VC dimension</li>
<li>VIGRA</li>
<li>Validation set</li>
<li>Vapnik–Chervonenkis theory</li>
<li>Variable-order Bayesian network</li>
<li>Variable kernel density estimation</li>
<li>Variable rules analysis</li>
<li>Variational message passing</li>
<li>Varimax rotation</li>
<li>Vector quantization</li>
<li>Vicarious (company)</li>
<li>Viterbi algorithm</li>
<li>Vowpal Wabbit</li>
<li>WACA clustering algorithm</li>
<li>WPGMA</li>
<li>Ward's method</li>
<li>Weasel program</li>
<li>Whitening transformation</li>
<li>Winnow (algorithm)</li>
<li>Win–stay, lose–switch</li>
<li>Witness set</li>
<li>Wolfram Language</li>
<li>Wolfram Mathematica</li>
<li>Writer invariant</li>
<li>Xgboost</li>
<li>Yooreeka</li>
<li>Zeroth (software)</li>
</ul>

<h2 id="14-further-reading">14 Further reading</h2>

<p>Trevor Hastie, Robert Tibshirani and Jerome H. Friedman (2001). The Elements of Statistical Learning, Springer. ISBN 0-387-95284-5.
Pedro Domingos (September 2015), The Master Algorithm, Basic Books, ISBN 978-0-465-06570-7
Mehryar Mohri, Afshin Rostamizadeh, Ameet Talwalkar (2012). Foundations of Machine Learning, The MIT Press. ISBN 978-0-262-01825-8.
Ian H. Witten and Eibe Frank (2011). Data Mining: Practical machine learning tools and techniques Morgan Kaufmann, 664pp., ISBN 978-0-12-374856-0.
David J. C. MacKay. Information Theory, Inference, and Learning Algorithms Cambridge: Cambridge University Press, 2003. ISBN 0-521-64298-1
Richard O. Duda, Peter E. Hart, David G. Stork (2001) Pattern classification (2nd edition), Wiley, New York, ISBN 0-471-05669-3.
Christopher Bishop (1995). Neural Networks for Pattern Recognition, Oxford University Press. ISBN 0-19-853864-2.
Vladimir Vapnik (1998). Statistical Learning Theory. Wiley-Interscience, ISBN 0-471-03003-1.
Ray Solomonoff, An Inductive Inference Machine, IRE Convention Record, Section on Information Theory, Part 2, pp., 56-62, 1957.
Ray Solomonoff, &quot;An Inductive Inference Machine&quot; A privately circulated report from the 1956 Dartmouth Summer Research Conference on AI.</p>

<h2 id="15-references">15 References</h2>

<p>^
Jump up to:
a b <a href="http://www.britannica.com/EBchecked/topic/1116194/machine-learning">http://www.britannica.com/EBchecked/topic/1116194/machine-learning</a>  This tertiary source reuses information from other sources but does not name them.
Jump up
^ Phil Simon (March 18, 2013). Too Big to Ignore: The Business Case for Big Data. Wiley. p. 89. ISBN 978-1-118-63817-0.
Jump up
^ Ron Kohavi; Foster Provost (1998). &quot;Glossary of terms&quot;. Machine Learning. 30: 271–274.
Jump up
^ <a href="http://www.learningtheory.org/">http://www.learningtheory.org/</a>
Jump up
^ Settles, Burr (2010), &quot;Active Learning Literature Survey&quot; (PDF), Computer Sciences Technical Report 1648. University of Wisconsin–Madison, retrieved 2014-11-18
Jump up
^ Rubens, Neil; Elahi, Mehdi; Sugiyama, Masashi; Kaplan, Dain (2016). &quot;Active Learning in Recommender Systems&quot;. In Ricci, Francesco; Rokach, Lior; Shapira, Bracha. Recommender Systems Handbook (2 ed.). Springer US. doi:10.1007/978-1-4899-7637-6. ISBN 978-1-4899-7637-6.
Jump up
^ <a href="https://en.wikipedia.org/wiki/Generative_adversarial_network#cite_note-GANs-1">https://en.wikipedia.org/wiki/Generative_adversarial_network#cite_note-GANs-1</a></p>

<h2 id="16-external-links">16 External links</h2>

<p>Data Science: Data to Insights from MIT (machine learning)
Popular online course by Andrew Ng, at Coursera. It uses GNU Octave. The course is a free version of Stanford University's actual course taught by Ng, see.stanford.edu/Course/CS229 available for free].
mloss is an academic database of open-source machine learning software.</p>

    </article>
  </div>

  
  
  
  <script>
  window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
  ga('create', 'UA-123456789-1', 'auto');
  ga('send', 'pageview');
  </script>
  <script async src='https://www.google-analytics.com/analytics.js'></script>
  

  
<link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.css" />
<script src="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.js"></script>
<script>
window.addEventListener("load", function(){
window.cookieconsent.initialise({
  "palette": {
    "popup": {
      "background": "#216942",
      "text": "#b2d192"
    },
    "button": {
      "background": "#afed71"
    }
  }
})});
</script>

</body>

</html>